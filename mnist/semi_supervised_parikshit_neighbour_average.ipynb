{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data/'\n",
    "checkpointDir = '../checkpoints/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),])\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=dataDir, train=True,  transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=dataDir, train=False, transform=transform)\n",
    "celoss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(42)\n",
    "\n",
    "allExamples = []#np.zeros((10000,28*28)).astype(np.float32)\n",
    "allLabels = []\n",
    "size = 10000\n",
    "unlab_size= 30000\n",
    "for x in train_set :\n",
    "    allExamples.append(x[0].flatten().numpy())\n",
    "    allLabels.append(x[1])        \n",
    "allExamples = np.array(allExamples)\n",
    "allLabels = np.array(allLabels)\n",
    "combinedExamples = np.concatenate([allExamples,allLabels[:,None]],axis=1)\n",
    "np.random.shuffle(combinedExamples)\n",
    "allExamples = combinedExamples[:size,:784]\n",
    "unlabExamples = combinedExamples[size:(size+unlab_size), :784]\n",
    "allLabels = combinedExamples[:size,-1]\n",
    "testExamples = []#np.zeros((10000,28*28)).astype(np.float32)\n",
    "testLabels = []\n",
    "\n",
    "for x in test_set :\n",
    "    testExamples.append(x[0].flatten().numpy())\n",
    "    testLabels.append(x[1])        \n",
    "testExamples = np.array(testExamples)\n",
    "testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "similarity = euclidean_distances(allExamples, unlabExamples)\n",
    "newExamples = []\n",
    "newLabels = []\n",
    "\n",
    "#for i in range(len(allExamples)):\n",
    "for i in np.where(num_tries.data.cpu().numpy()>0)[0]:\n",
    "    indices = np.argsort(similarity[i])[:20]\n",
    "    newExamples.append(unlabExamples[indices].mean(axis=0)[:])\n",
    "    newLabels.append(allLabels[i])\n",
    "    \n",
    "allExamples = np.concatenate([allExamples,np.array(newExamples)],axis=0)\n",
    "allLabels = np.concatenate([allLabels,np.array(newLabels)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12747, 784)\n",
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "print (allExamples.shape)\n",
    "print(unlabExamples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedExamples = np.concatenate([allExamples,allLabels[:,None]],axis=1)\n",
    "# combinedExamples = combinedExamples[combinedExamples[:,-1].argsort()]\n",
    "# newExamples = []\n",
    "# newLabels = []\n",
    "\n",
    "# for i in range(len(combinedExamples)-1):\n",
    "#     newExamples.append((combinedExamples[i,:784]+combinedExamples[i+1,:784])/2)\n",
    "#     newLabels.append(combinedExamples[i,-1])\n",
    "    \n",
    "# newExamples.append(combinedExamples[i+1,:784])\n",
    "# newLabels.append(combinedExamples[i+1,-1])\n",
    "\n",
    "# allExamples = np.concatenate([allExamples,np.array(newExamples)],axis=0)\n",
    "# allLabels = np.concatenate([allLabels,np.array(newLabels)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = model.BasicNN().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(log_dir = 'runs/run10K_unlabforgetting')\n",
    "#writer = SummaryWriter(log_dir = 'runs/run10K_10KEUsimilarity')\n",
    "\n",
    "batch_size = 100\n",
    "max_epoch = 200\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_tensor = torch.cuda.FloatTensor([epoch])\n",
    "    shuff = torch.from_numpy(np.random.permutation(np.arange(len(allExamples))))\n",
    "    net.train()\n",
    "    for i in range(0,len(allExamples),batch_size):\n",
    "        batch_ind = shuff[i:min(i+batch_size,len(allExamples))]\n",
    "        inputs = torch.from_numpy(allExamples[batch_ind]).float().cuda()\n",
    "        targets = torch.LongTensor(allLabels[batch_ind]).cuda()\n",
    "        outputs = net(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = celoss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_,acc = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(testExamples),batch_size):\n",
    "            batch_ind = np.arange(batch_size)+i\n",
    "            inputs = torch.from_numpy(testExamples[batch_ind]).float().cuda()\n",
    "            targets = torch.LongTensor(testLabels[batch_ind]).cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            acc += (predicted==targets).sum().float()\n",
    "            loss_ += celoss(outputs, targets).data*inputs.shape[0]\n",
    "    writer.add_scalar('test/loss',loss_/len(testExamples),epoch)\n",
    "    writer.add_scalar('test/acc',acc/len(testExamples),epoch)\n",
    "    val_loss = loss_/len(testExamples)\n",
    "    loss_,acc = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(allExamples),batch_size):\n",
    "            batch_ind = shuff[i:min(i+batch_size,len(allExamples))]\n",
    "            inputs = torch.from_numpy(allExamples[batch_ind]).float().cuda()\n",
    "            targets = torch.LongTensor(allLabels[batch_ind]).cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            acc += (predicted==targets).sum().float()\n",
    "            loss_ += celoss(outputs, targets).data*inputs.shape[0]\n",
    "    writer.add_scalar('train/loss',loss_/len(allExamples),epoch)\n",
    "    writer.add_scalar('train/acc',acc/len(allExamples),epoch)\n",
    "    writer.add_scalar('generalisation_error',(val_loss-loss_/len(allExamples)),epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# binaryExamples = []#np.zeros((10000,28*28)).astype(np.float32)\n",
    "# binaryLabels = []\n",
    "\n",
    "# for x in train_set :\n",
    "#     if (x[1] == 0 or x[1] == 1):\n",
    "#         binaryExamples.append(x[0].flatten().numpy())\n",
    "#         binaryLabels.append(x[1])        \n",
    "# binaryExamples = np.array(binaryExamples)\n",
    "# binaryLabels = np.array(binaryLabels)\n",
    "\n",
    "# indices = np.concatenate([np.where(binaryLabels==0)[0][:5000],np.where(binaryLabels==1)[0][:5000]])\n",
    "# binaryExamples = binaryExamples[indices]\n",
    "# binaryLabels = binaryLabels[indices]\n",
    "\n",
    "\n",
    "predictions = torch.from_numpy(np.zeros(len(allExamples))).float().cuda()\n",
    "num_tries = torch.from_numpy(np.zeros(len(allExamples))).float().cuda()\n",
    "first_learning = torch.from_numpy(np.zeros(len(allExamples))).float().cuda()+np.inf\n",
    "inf_tensor = torch.cuda.FloatTensor([float('inf')])\n",
    "\n",
    "net = model.BasicNN().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "batch_size = 100\n",
    "max_epoch = 200\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_tensor = torch.cuda.FloatTensor([epoch])\n",
    "    shuff = torch.from_numpy(np.random.permutation(np.arange(len(allExamples))))\n",
    "    net.train()\n",
    "    for i in range(0,len(allExamples),batch_size):\n",
    "        batch_ind = shuff[i:i+batch_size]\n",
    "        inputs = torch.from_numpy(allExamples[batch_ind]).float().cuda()\n",
    "        targets = torch.LongTensor(allLabels[batch_ind]).cuda()\n",
    "        outputs = net(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = celoss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ###########this part maintains forgetting stats#################                                                                                                                                                                  \n",
    "        _, predicted = outputs.max(1)\n",
    "        old_predictions = predictions[batch_ind]\n",
    "        new_predictions = predicted.eq(targets).float()\n",
    "        diff_pred = old_predictions - new_predictions\n",
    "        num_tries[batch_ind[(diff_pred > 0).nonzero()]] += 1\n",
    "        predictions[batch_ind] = new_predictions\n",
    "        ###########this part maintains first learning event##############                                                                                                                                                                 \n",
    "        new_predictions = torch.where(new_predictions == 1,epoch_tensor,inf_tensor)\n",
    "        first_learning[batch_ind],_ = torch.min(torch.stack((first_learning[batch_ind],new_predictions)),dim=0)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
