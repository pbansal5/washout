{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '../data/'\n",
    "checkpointDir = '../checkpoints/'\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),])\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=dataDir, train=True,  transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=dataDir, train=False, transform=transform)\n",
    "celoss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(42)\n",
    "\n",
    "allExamples = []#np.zeros((10000,28*28)).astype(np.float32)\n",
    "allLabels = []\n",
    "size = 10000\n",
    "unlab_size= 30000\n",
    "for x in train_set :\n",
    "    allExamples.append(x[0].flatten().numpy())\n",
    "    allLabels.append(x[1])        \n",
    "allExamples = np.array(allExamples)\n",
    "allLabels = np.array(allLabels)\n",
    "combinedExamples = np.concatenate([allExamples,allLabels[:,None]],axis=1)\n",
    "np.random.shuffle(combinedExamples)\n",
    "allExamples = combinedExamples[:size,:784]\n",
    "unlabExamples = combinedExamples[size:(size+unlab_size), :784]\n",
    "allLabels = combinedExamples[:size,-1]\n",
    "testExamples = []#np.zeros((10000,28*28)).astype(np.float32)\n",
    "testLabels = []\n",
    "\n",
    "for x in test_set :\n",
    "    testExamples.append(x[0].flatten().numpy())\n",
    "    testLabels.append(x[1])        \n",
    "testExamples = np.array(testExamples)\n",
    "testLabels = np.array(testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_unlab= forget_unlab.cpu().numpy()\n",
    "label_cnt= label_cnt.cpu().numpy()\n",
    "chosen_unlab = (forget_unlab>1)\n",
    "allExamples = np.concatenate([allExamples,unlabExamples[chosen_unlab,:]],axis=0)\n",
    "allLabels = np.concatenate([allLabels, np.argmax(label_cnt[chosen_unlab,:], axis=1)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20558, 784)\n",
      "(30000, 784)\n"
     ]
    }
   ],
   "source": [
    "print (allExamples.shape)\n",
    "print(unlabExamples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combinedExamples = np.concatenate([allExamples,allLabels[:,None]],axis=1)\n",
    "# combinedExamples = combinedExamples[combinedExamples[:,-1].argsort()]\n",
    "# newExamples = []\n",
    "# newLabels = []\n",
    "\n",
    "# for i in range(len(combinedExamples)-1):\n",
    "#     newExamples.append((combinedExamples[i,:784]+combinedExamples[i+1,:784])/2)\n",
    "#     newLabels.append(combinedExamples[i,-1])\n",
    "    \n",
    "# newExamples.append(combinedExamples[i+1,:784])\n",
    "# newLabels.append(combinedExamples[i+1,-1])\n",
    "\n",
    "# allExamples = np.concatenate([allExamples,np.array(newExamples)],axis=0)\n",
    "# allLabels = np.concatenate([allLabels,np.array(newLabels)],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = model.BasicNN().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(log_dir = 'runs/run10K_forgetting_stats_on_unlabelled')\n",
    "#writer = SummaryWriter(log_dir = 'runs/run10K_10KEUsimilarity')\n",
    "\n",
    "batch_size = 100\n",
    "max_epoch = 200\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_tensor = torch.cuda.FloatTensor([epoch])\n",
    "    shuff = torch.from_numpy(np.random.permutation(np.arange(len(allExamples))))\n",
    "    net.train()\n",
    "    for i in range(0,len(allExamples),batch_size):\n",
    "        batch_ind = shuff[i:min(i+batch_size,len(allExamples))]\n",
    "        inputs = torch.from_numpy(allExamples[batch_ind]).float().cuda()\n",
    "        targets = torch.LongTensor(allLabels[batch_ind]).cuda()\n",
    "        outputs = net(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = celoss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    loss_,acc = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(testExamples),batch_size):\n",
    "            batch_ind = np.arange(batch_size)+i\n",
    "            inputs = torch.from_numpy(testExamples[batch_ind]).float().cuda()\n",
    "            targets = torch.LongTensor(testLabels[batch_ind]).cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            acc += (predicted==targets).sum().float()\n",
    "            loss_ += celoss(outputs, targets).data*inputs.shape[0]\n",
    "    writer.add_scalar('test/loss',loss_/len(testExamples),epoch)\n",
    "    writer.add_scalar('test/acc',acc/len(testExamples),epoch)\n",
    "    val_loss = loss_/len(testExamples)\n",
    "    loss_,acc = 0,0\n",
    "    with torch.no_grad():\n",
    "        for i in range(0,len(allExamples),batch_size):\n",
    "            batch_ind = shuff[i:min(i+batch_size,len(allExamples))]\n",
    "            inputs = torch.from_numpy(allExamples[batch_ind]).float().cuda()\n",
    "            targets = torch.LongTensor(allLabels[batch_ind]).cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            acc += (predicted==targets).sum().float()\n",
    "            loss_ += celoss(outputs, targets).data*inputs.shape[0]\n",
    "    writer.add_scalar('train/loss',loss_/len(allExamples),epoch)\n",
    "    writer.add_scalar('train/acc',acc/len(allExamples),epoch)\n",
    "    writer.add_scalar('generalisation_error',(val_loss-loss_/len(allExamples)),epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6aad32b5b0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(np.array(test_loss))\n",
    "# plt.plot(np.array(train_loss))\n",
    "plt.plot(np.array(test_loss)-np.array(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# binaryExamples = []#np.zeros((10000,28*28)).astype(np.float32)\n",
    "# binaryLabels = []\n",
    "\n",
    "# for x in train_set :\n",
    "#     if (x[1] == 0 or x[1] == 1):\n",
    "#         binaryExamples.append(x[0].flatten().numpy())\n",
    "#         binaryLabels.append(x[1])        \n",
    "# binaryExamples = np.array(binaryExamples)\n",
    "# binaryLabels = np.array(binaryLabels)\n",
    "\n",
    "# indices = np.concatenate([np.where(binaryLabels==0)[0][:5000],np.where(binaryLabels==1)[0][:5000]])\n",
    "# binaryExamples = binaryExamples[indices]\n",
    "# binaryLabels = binaryLabels[indices]\n",
    "\n",
    "\n",
    "predictions = torch.from_numpy(np.zeros(len(allExamples))).float().cuda()\n",
    "num_tries = torch.from_numpy(np.zeros(len(allExamples))).float().cuda()\n",
    "first_learning = torch.from_numpy(np.zeros(len(allExamples))).float().cuda()+np.inf\n",
    "inf_tensor = torch.cuda.FloatTensor([float('inf')])\n",
    "\n",
    "net = model.BasicNN().cuda()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "batch_size = 100\n",
    "max_epoch = 200\n",
    "test_loss = []\n",
    "train_loss = []\n",
    "prev_unlab_pred = torch.from_numpy(np.zeros(len(unlabExamples))).long().cuda()\n",
    "forget_unlab = torch.from_numpy(np.zeros(len(unlabExamples))).float().cuda()\n",
    "label_cnt = torch.from_numpy(np.zeros((len(unlabExamples),10))).float().cuda()\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    epoch_tensor = torch.cuda.FloatTensor([epoch])\n",
    "    shuff = torch.from_numpy(np.random.permutation(np.arange(len(allExamples))))\n",
    "    net.train()\n",
    "    for i in range(0,len(allExamples),batch_size):\n",
    "        batch_ind = shuff[i:i+batch_size]\n",
    "        inputs = torch.from_numpy(allExamples[batch_ind]).float().cuda()\n",
    "        targets = torch.LongTensor(allLabels[batch_ind]).cuda()\n",
    "        outputs = net(inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss = celoss(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ###########this part maintains forgetting stats#################                                                                                                                                                                  \n",
    "        _, predicted = outputs.max(1)\n",
    "        old_predictions = predictions[batch_ind]\n",
    "        new_predictions = predicted.eq(targets).float()\n",
    "        diff_pred = old_predictions - new_predictions\n",
    "        num_tries[batch_ind[(diff_pred > 0).nonzero()]] += 1\n",
    "        predictions[batch_ind] = new_predictions\n",
    "        ###########this part maintains first learning event##############                                                                                                                                                                 \n",
    "        new_predictions = torch.where(new_predictions == 1,epoch_tensor,inf_tensor)\n",
    "        first_learning[batch_ind],_ = torch.min(torch.stack((first_learning[batch_ind],new_predictions)),dim=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for j in range(0,len(unlabExamples),batch_size):\n",
    "            unlab_ind = torch.from_numpy(np.asarray(list(range(j,(j+min(batch_size, len(unlabExamples)))))))\n",
    "            inputs = torch.from_numpy(unlabExamples[unlab_ind]).float().cuda()\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            if i>0:\n",
    "                ind = unlab_ind[predicted != prev_unlab_pred[unlab_ind]]\n",
    "                forget_unlab[ind] += 1                \n",
    "            label_cnt[unlab_ind,predicted] += 1\n",
    "            prev_unlab_pred[unlab_ind] = predicted\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
